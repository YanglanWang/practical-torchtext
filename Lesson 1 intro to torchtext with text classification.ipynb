{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's examine what the data looks like.\n",
    "\n",
    "(Note: This repo does not contain the full data. To get the full data, go to the [Kaggle competition page](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and download the data for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/train.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000eefc67a2c930f</td>\n",
       "      <td>Radial symmetry \\n\\nSeveral now extinct lineag...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000f35deef84dc4a</td>\n",
       "      <td>There's no need to apologize. A Wikipedia arti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000ffab30195c5e1</td>\n",
       "      <td>Yes, because the mother of the child in the ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  000eefc67a2c930f  Radial symmetry \\n\\nSeveral now extinct lineag...      0   \n",
       "1  000f35deef84dc4a  There's no need to apologize. A Wikipedia arti...      0   \n",
       "2  000ffab30195c5e1  Yes, because the mother of the child in the ca...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/valid.csv\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently we have to predict 6 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/test.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Field class determines how the data is preprocessed and converted into a numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want comment_text field to be converted to lowercase, tokenized on whitespace, and preprocessed. So we tell that to the Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple. The preprocessing of the labels is even easier, since they are already converted into a binary encoding.\n",
    "All we need to do is to tell the Field class that the labels are already processed. We do this by passing the use_vocab=False keyword to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the TabularDataset class to read our data, since it is in csv format (TabularDataset handles csv, tsv, and json files as of now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train and validation data, we need to process the labels. The fields we pass in must be in the same order as the columns. For fields we don't use, we pass in a tuple where the second element is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.27 ms, sys: 39 µs, total: 4.31 ms\n",
      "Wall time: 3.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n",
    "                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n",
    "                 (\"obscene\", LABEL), (\"insult\", LABEL),\n",
    "                 (\"identity_hate\", LABEL)]\n",
    "\n",
    "trn, vld = TabularDataset.splits(\n",
    "        path=\"data\", # the root directory where the data lies\n",
    "        train='train.csv', validation=\"valid.csv\",\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test data, we don't have any labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.54 ms, sys: 0 ns, total: 3.54 ms\n",
      "Wall time: 2.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tst_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT)\n",
    "]\n",
    "\n",
    "tst = TabularDataset(\n",
    "        path=\"data/test.csv\", # the file path\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tst_datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the TEXT field to convert words into integers, it needs to be told what the entire vocabulary is. To do this, we run TEXT.build_vocab, passing in the dataset to build the vocabulary on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 ms, sys: 0 ns, total: 2.28 ms\n",
      "Wall time: 2.29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the vocab looks like.\n",
    "\n",
    "The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index()>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'the': 2,\n",
       "             'to': 3,\n",
       "             'you': 4,\n",
       "             'of': 5,\n",
       "             'a': 6,\n",
       "             'and': 7,\n",
       "             'is': 8,\n",
       "             'that': 9,\n",
       "             'i': 10,\n",
       "             'if': 11,\n",
       "             'be': 12,\n",
       "             'in': 13,\n",
       "             'this': 14,\n",
       "             '\"': 15,\n",
       "             'have': 16,\n",
       "             'it': 17,\n",
       "             'on': 18,\n",
       "             'at': 19,\n",
       "             'for': 20,\n",
       "             'use': 21,\n",
       "             'are': 22,\n",
       "             'not': 23,\n",
       "             'any': 24,\n",
       "             'as': 25,\n",
       "             'can': 26,\n",
       "             'he': 27,\n",
       "             'copyright': 28,\n",
       "             'fair': 29,\n",
       "             \"i'm\": 30,\n",
       "             'or': 31,\n",
       "             'page': 32,\n",
       "             'please': 33,\n",
       "             'then': 34,\n",
       "             'with': 35,\n",
       "             'about': 36,\n",
       "             'his': 37,\n",
       "             'me': 38,\n",
       "             'one': 39,\n",
       "             'an': 40,\n",
       "             'do': 41,\n",
       "             'edit': 42,\n",
       "             'image': 43,\n",
       "             'your': 44,\n",
       "             'from': 45,\n",
       "             'more': 46,\n",
       "             'my': 47,\n",
       "             'so': 48,\n",
       "             'under': 49,\n",
       "             'was': 50,\n",
       "             'after': 51,\n",
       "             'article': 52,\n",
       "             'been': 53,\n",
       "             'but': 54,\n",
       "             \"don't\": 55,\n",
       "             'questions': 56,\n",
       "             'there': 57,\n",
       "             'they': 58,\n",
       "             'will': 59,\n",
       "             '-': 60,\n",
       "             'articles': 61,\n",
       "             'ask': 62,\n",
       "             'before': 63,\n",
       "             'by': 64,\n",
       "             'edits': 65,\n",
       "             'explanation': 66,\n",
       "             'look': 67,\n",
       "             'no': 68,\n",
       "             'other': 69,\n",
       "             'page.': 70,\n",
       "             'rationale': 71,\n",
       "             'some': 72,\n",
       "             'talk': 73,\n",
       "             'them': 74,\n",
       "             'think': 75,\n",
       "             'uploaded': 76,\n",
       "             'use.': 77,\n",
       "             'what': 78,\n",
       "             'when': 79,\n",
       "             'which': 80,\n",
       "             'who': 81,\n",
       "             'why': 82,\n",
       "             'would': 83,\n",
       "             '•': 84,\n",
       "             'also': 85,\n",
       "             'anything': 86,\n",
       "             'argument': 87,\n",
       "             'around': 88,\n",
       "             'believe': 89,\n",
       "             'criteria': 90,\n",
       "             'deleted': 91,\n",
       "             'description': 92,\n",
       "             \"doesn't\": 93,\n",
       "             'how': 94,\n",
       "             'include': 95,\n",
       "             \"it's\": 96,\n",
       "             'juelz': 97,\n",
       "             'law': 98,\n",
       "             'like': 99,\n",
       "             'list': 100,\n",
       "             'listed': 101,\n",
       "             'makes': 102,\n",
       "             'may': 103,\n",
       "             'media': 104,\n",
       "             'need': 105,\n",
       "             'really': 106,\n",
       "             'see': 107,\n",
       "             'should': 108,\n",
       "             'tag': 109,\n",
       "             'tags': 110,\n",
       "             'than': 111,\n",
       "             'their': 112,\n",
       "             'those': 113,\n",
       "             'wikipedia': 114,\n",
       "             'years': 115,\n",
       "             'you.': 116,\n",
       "             '(talk': 117,\n",
       "             ')': 118,\n",
       "             '.': 119,\n",
       "             'academic': 120,\n",
       "             'accusations': 121,\n",
       "             'again,': 122,\n",
       "             'always': 123,\n",
       "             'catagory': 124,\n",
       "             'checking': 125,\n",
       "             'consider': 126,\n",
       "             'content,': 127,\n",
       "             'contribs': 128,\n",
       "             'could': 129,\n",
       "             'deletion.': 130,\n",
       "             'described': 131,\n",
       "             'did': 132,\n",
       "             'discuss': 133,\n",
       "             'edit?': 134,\n",
       "             'even': 135,\n",
       "             'file': 136,\n",
       "             'find': 137,\n",
       "             'first': 138,\n",
       "             'following': 139,\n",
       "             'format': 140,\n",
       "             'formatting': 141,\n",
       "             'full': 142,\n",
       "             'go': 143,\n",
       "             'good': 144,\n",
       "             'harlan': 145,\n",
       "             'has': 146,\n",
       "             'her': 147,\n",
       "             'image:wonju.jpg': 148,\n",
       "             'image:wonju.jpg.': 149,\n",
       "             'images': 150,\n",
       "             'info.': 151,\n",
       "             'issue': 152,\n",
       "             'its': 153,\n",
       "             'john': 154,\n",
       "             'just': 155,\n",
       "             'know.': 156,\n",
       "             'later': 157,\n",
       "             'lets': 158,\n",
       "             'link': 159,\n",
       "             'made': 160,\n",
       "             'make': 161,\n",
       "             'marshall': 162,\n",
       "             'me,': 163,\n",
       "             'men': 164,\n",
       "             'merely': 165,\n",
       "             'most': 166,\n",
       "             'must': 167,\n",
       "             'neff': 168,\n",
       "             'new': 169,\n",
       "             'note': 170,\n",
       "             'on,': 171,\n",
       "             'out': 172,\n",
       "             'pages': 173,\n",
       "             'paragraph': 174,\n",
       "             'read': 175,\n",
       "             'recent': 176,\n",
       "             'references': 177,\n",
       "             'regarding': 178,\n",
       "             'relevant': 179,\n",
       "             'removing': 180,\n",
       "             'review': 181,\n",
       "             'she': 182,\n",
       "             'simply': 183,\n",
       "             'since': 184,\n",
       "             'someone': 185,\n",
       "             'source': 186,\n",
       "             'specific': 187,\n",
       "             'specified': 188,\n",
       "             'specify': 189,\n",
       "             'speedy': 190,\n",
       "             'stop': 191,\n",
       "             'stuck': 192,\n",
       "             'such': 193,\n",
       "             'take': 194,\n",
       "             'thank': 195,\n",
       "             'thanks': 196,\n",
       "             \"that's\": 197,\n",
       "             'together': 198,\n",
       "             'too.': 199,\n",
       "             'up': 200,\n",
       "             'uploading': 201,\n",
       "             'used': 202,\n",
       "             'very': 203,\n",
       "             'want': 204,\n",
       "             'we': 205,\n",
       "             \"website's\": 206,\n",
       "             'week': 207,\n",
       "             'well': 208,\n",
       "             'where': 209,\n",
       "             'wikipedia:fair': 210,\n",
       "             'wikipedia:image': 211,\n",
       "             'write': 212,\n",
       "             '\"\"a': 213,\n",
       "             '\"\"down\"\".': 214,\n",
       "             '\"\"enhanced;\"\"': 215,\n",
       "             '\"\"image\"\"': 216,\n",
       "             '\"\"my': 217,\n",
       "             '\"\"santana\\'s': 218,\n",
       "             '\"\"the': 219,\n",
       "             '\"\"types': 220,\n",
       "             '\"\"wrong.\"\"': 221,\n",
       "             \"'45\": 222,\n",
       "             \"'animal\": 223,\n",
       "             \"'image'\": 224,\n",
       "             \"'nonsense'\": 225,\n",
       "             \"'s\": 226,\n",
       "             '(forgive': 227,\n",
       "             '(it': 228,\n",
       "             '(juelz': 229,\n",
       "             '(non)-contribution': 230,\n",
       "             '(per': 231,\n",
       "             '(talk)': 232,\n",
       "             '(utc)': 233,\n",
       "             \"-''''''the\": 234,\n",
       "             '-i': 235,\n",
       "             '11,': 236,\n",
       "             '18': 237,\n",
       "             '18th,': 238,\n",
       "             '19': 239,\n",
       "             '1983': 240,\n",
       "             '1983.': 241,\n",
       "             '1st': 242,\n",
       "             '20': 243,\n",
       "             '2002,': 244,\n",
       "             '2003,': 245,\n",
       "             '2006': 246,\n",
       "             '2006,': 247,\n",
       "             '2012': 248,\n",
       "             '2016': 249,\n",
       "             '21:51,': 250,\n",
       "             '22': 251,\n",
       "             '23': 252,\n",
       "             '23.': 253,\n",
       "             '24': 254,\n",
       "             '4': 255,\n",
       "             '48': 256,\n",
       "             '93.161.107.169': 257,\n",
       "             '@': 258,\n",
       "             '[': 259,\n",
       "             'a1.': 260,\n",
       "             'a2.': 261,\n",
       "             'above': 262,\n",
       "             'absence': 263,\n",
       "             'abt': 264,\n",
       "             'abusing': 265,\n",
       "             'accidents\"\"': 266,\n",
       "             'according': 267,\n",
       "             'acctually': 268,\n",
       "             'achieve': 269,\n",
       "             'acknowledged.': 270,\n",
       "             'actual': 271,\n",
       "             'ad': 272,\n",
       "             'add': 273,\n",
       "             'adding': 274,\n",
       "             'addition': 275,\n",
       "             'admin': 276,\n",
       "             'admin.': 277,\n",
       "             'age': 278,\n",
       "             'aircracft': 279,\n",
       "             'alignment': 280,\n",
       "             'all': 281,\n",
       "             'almost': 282,\n",
       "             'already.': 283,\n",
       "             'anecdote': 284,\n",
       "             'anti-social': 285,\n",
       "             'antonin': 286,\n",
       "             'anyone': 287,\n",
       "             'anything)\"': 288,\n",
       "             'anyway,': 289,\n",
       "             'apologized.': 290,\n",
       "             'appears': 291,\n",
       "             'approach': 292,\n",
       "             'argue': 293,\n",
       "             'argument,': 294,\n",
       "             'arguments': 295,\n",
       "             'arms.\"\"': 296,\n",
       "             'article(wow': 297,\n",
       "             'articles.': 298,\n",
       "             'asks': 299,\n",
       "             'assertion': 300,\n",
       "             'attacks': 301,\n",
       "             'attend.': 302,\n",
       "             'attention': 303,\n",
       "             'attracting': 304,\n",
       "             'audio,': 305,\n",
       "             'avoided': 306,\n",
       "             'b1.': 307,\n",
       "             'b2.': 308,\n",
       "             'back!': 309,\n",
       "             'background': 310,\n",
       "             'backlog': 311,\n",
       "             'bad': 312,\n",
       "             'banks?': 313,\n",
       "             'banned.': 314,\n",
       "             'bbq': 315,\n",
       "             'because': 316,\n",
       "             'becomes:': 317,\n",
       "             'behavior': 318,\n",
       "             'being': 319,\n",
       "             'belong': 320,\n",
       "             'belong.': 321,\n",
       "             'benjamin': 322,\n",
       "             'besides': 323,\n",
       "             'better.': 324,\n",
       "             'between': 325,\n",
       "             'birth.': 326,\n",
       "             'birthday': 327,\n",
       "             'boilerplate': 328,\n",
       "             'born': 329,\n",
       "             'bother': 330,\n",
       "             'box.': 331,\n",
       "             'breeding': 332,\n",
       "             \"breeding'\": 333,\n",
       "             'bye!': 334,\n",
       "             'caculator': 335,\n",
       "             \"cam's\": 336,\n",
       "             'came': 337,\n",
       "             \"can't\": 338,\n",
       "             'cardozo': 339,\n",
       "             'care': 340,\n",
       "             'carrier.': 341,\n",
       "             'catagory.': 342,\n",
       "             'certainly': 343,\n",
       "             'chance': 344,\n",
       "             'changes': 345,\n",
       "             'changing': 346,\n",
       "             'chernodrinski': 347,\n",
       "             'civility': 348,\n",
       "             'clarence': 349,\n",
       "             'clean': 350,\n",
       "             'clicking': 351,\n",
       "             'closer': 352,\n",
       "             'closure': 353,\n",
       "             'cocksucker': 354,\n",
       "             'colour': 355,\n",
       "             'come': 356,\n",
       "             'coming': 357,\n",
       "             'comming': 358,\n",
       "             'common': 359,\n",
       "             'congratulations': 360,\n",
       "             'consistent': 361,\n",
       "             'constantly': 362,\n",
       "             'constitutes': 363,\n",
       "             'contrary': 364,\n",
       "             'contributions\"\"': 365,\n",
       "             'copyright.': 366,\n",
       "             'copyrighted': 367,\n",
       "             'create': 368,\n",
       "             'created': 369,\n",
       "             'created/took': 370,\n",
       "             'credentials': 371,\n",
       "             'crystals': 372,\n",
       "             'current': 373,\n",
       "             'currently': 374,\n",
       "             \"d'aww!\": 375,\n",
       "             'damage.': 376,\n",
       "             'data': 377,\n",
       "             'date': 378,\n",
       "             'death,': 379,\n",
       "             'deaths': 380,\n",
       "             'decent': 381,\n",
       "             'dedicating': 382,\n",
       "             'degree': 383,\n",
       "             'delay': 384,\n",
       "             'delting': 385,\n",
       "             'deserves': 386,\n",
       "             'destroying,': 387,\n",
       "             'destructive': 388,\n",
       "             'details': 389,\n",
       "             'different': 390,\n",
       "             'diplomats.': 391,\n",
       "             'discussion': 392,\n",
       "             'does': 393,\n",
       "             'dolls': 394,\n",
       "             'dont': 395,\n",
       "             'dropdown': 396,\n",
       "             'dulithgow': 397,\n",
       "             'each': 398,\n",
       "             'edit,': 399,\n",
       "             'edited': 400,\n",
       "             'editing': 401,\n",
       "             'eg': 402,\n",
       "             'either': 403,\n",
       "             'else': 404,\n",
       "             'encyclopedic': 405,\n",
       "             'entirely': 406,\n",
       "             'equals': 407,\n",
       "             'etc.': 408,\n",
       "             'eugenics?': 409,\n",
       "             'event': 410,\n",
       "             'ex-president': 411,\n",
       "             'exact': 412,\n",
       "             'exclusive': 413,\n",
       "             'expertise': 414,\n",
       "             'fac.': 415,\n",
       "             'facts': 416,\n",
       "             'faculty': 417,\n",
       "             'falsities': 418,\n",
       "             'fan': 419,\n",
       "             'far': 420,\n",
       "             'february': 421,\n",
       "             'fella.': 422,\n",
       "             \"file's\": 423,\n",
       "             'files': 424,\n",
       "             'files,': 425,\n",
       "             'film': 426,\n",
       "             'forbid': 427,\n",
       "             'form': 428,\n",
       "             'further': 429,\n",
       "             'gang': 430,\n",
       "             'gas': 431,\n",
       "             'gently': 432,\n",
       "             'geometry': 433,\n",
       "             'georgiev-': 434,\n",
       "             'get': 435,\n",
       "             'gfdl.': 436,\n",
       "             'girl': 437,\n",
       "             'gives': 438,\n",
       "             'god': 439,\n",
       "             'god.\"': 440,\n",
       "             'going': 441,\n",
       "             'good,': 442,\n",
       "             'grasp': 443,\n",
       "             'group': 444,\n",
       "             'guess': 445,\n",
       "             'guy': 446,\n",
       "             'hand': 447,\n",
       "             'hardcore': 448,\n",
       "             'harvard': 449,\n",
       "             'helps': 450,\n",
       "             'here': 451,\n",
       "             'hero.': 452,\n",
       "             'hey': 453,\n",
       "             'hey...': 454,\n",
       "             'hindi': 455,\n",
       "             'holder': 456,\n",
       "             'homie': 457,\n",
       "             'hominem': 458,\n",
       "             'honors': 459,\n",
       "             'hope': 460,\n",
       "             'hopes': 461,\n",
       "             'hours': 462,\n",
       "             'however,': 463,\n",
       "             'http://www.its.caltech.edu/~atomic/snowcrystals/myths/myths.htm#perfection': 464,\n",
       "             'humorous,': 465,\n",
       "             \"i'd\": 466,\n",
       "             'ie': 467,\n",
       "             'ii': 468,\n",
       "             'illustrates': 469,\n",
       "             'im': 470,\n",
       "             'improved': 471,\n",
       "             'improvement': 472,\n",
       "             'improvement.': 473,\n",
       "             'in),': 474,\n",
       "             'information': 475,\n",
       "             'information.': 476,\n",
       "             'information?': 477,\n",
       "             'instead': 478,\n",
       "             'int': 479,\n",
       "             'intend': 480,\n",
       "             'intending': 481,\n",
       "             'introducing': 482,\n",
       "             'introductory': 483,\n",
       "             'invite': 484,\n",
       "             'irregular': 485,\n",
       "             'is,': 486,\n",
       "             \"isn't\": 487,\n",
       "             'it,': 488,\n",
       "             'it-maybe': 489,\n",
       "             'it..': 490,\n",
       "             'it...': 491,\n",
       "             'it?': 492,\n",
       "             'itself': 493,\n",
       "             'itself,': 494,\n",
       "             'itself-making': 495,\n",
       "             'january': 496,\n",
       "             'judging?': 497,\n",
       "             'jump': 498,\n",
       "             'jurists.': 499,\n",
       "             'justices': 500,\n",
       "             'kathleen': 501,\n",
       "             'kenneth': 502,\n",
       "             'label': 503,\n",
       "             'lacking': 504,\n",
       "             'late.': 505,\n",
       "             'learned': 506,\n",
       "             'learning': 507,\n",
       "             'less': 508,\n",
       "             'let': 509,\n",
       "             'level': 510,\n",
       "             'libbrecht,': 511,\n",
       "             'liberally.': 512,\n",
       "             'license': 513,\n",
       "             'licensing': 514,\n",
       "             'link].': 515,\n",
       "             'living': 516,\n",
       "             'lloyd': 517,\n",
       "             'located': 518,\n",
       "             'logged': 519,\n",
       "             'look,': 520,\n",
       "             'making': 521,\n",
       "             'man': 522,\n",
       "             'man,': 523,\n",
       "             'managed': 524,\n",
       "             'many': 525,\n",
       "             'matches': 526,\n",
       "             'matt': 527,\n",
       "             'matter': 528,\n",
       "             'may,': 529,\n",
       "             'me.': 530,\n",
       "             'mean': 531,\n",
       "             'media,': 532,\n",
       "             'meets': 533,\n",
       "             'mentions': 534,\n",
       "             'messy': 535,\n",
       "             'metallica': 536,\n",
       "             'minimum,': 537,\n",
       "             'mitsurugi': 538,\n",
       "             'necessarily': 539,\n",
       "             'news': 540,\n",
       "             'no-one': 541,\n",
       "             'non-free': 542,\n",
       "             'nonsensical': 543,\n",
       "             'nose': 544,\n",
       "             'not.': 545,\n",
       "             'notable': 546,\n",
       "             'notable,': 547,\n",
       "             'notice': 548,\n",
       "             'noticed': 549,\n",
       "             'now,': 550,\n",
       "             'now.(deepu)': 551,\n",
       "             'now.89.205.38.27': 552,\n",
       "             'number': 553,\n",
       "             'obtained': 554,\n",
       "             'off': 555,\n",
       "             'offensive': 556,\n",
       "             'oh': 557,\n",
       "             'oh,': 558,\n",
       "             'old': 559,\n",
       "             'old,': 560,\n",
       "             'old.': 561,\n",
       "             'older': 562,\n",
       "             'on?': 563,\n",
       "             'once': 564,\n",
       "             'only': 565,\n",
       "             'others': 566,\n",
       "             'over': 567,\n",
       "             'own': 568,\n",
       "             'owner': 569,\n",
       "             'pair': 570,\n",
       "             'pairs': 571,\n",
       "             'paragraph?': 572,\n",
       "             'passed?': 573,\n",
       "             'peacefully.': 574,\n",
       "             'perhaps': 575,\n",
       "             'perhaps,': 576,\n",
       "             'persuaded': 577,\n",
       "             'phone?': 578,\n",
       "             'picture,': 579,\n",
       "             'piss': 580,\n",
       "             'point': 581,\n",
       "             'point.': 582,\n",
       "             'points': 583,\n",
       "             'pop': 584,\n",
       "             'posted': 585,\n",
       "             'power': 586,\n",
       "             'preferences': 587,\n",
       "             'premature': 588,\n",
       "             'probably': 589,\n",
       "             'processes': 590,\n",
       "             'proper': 591,\n",
       "             'publisher,': 592,\n",
       "             'purist': 593,\n",
       "             'question': 594,\n",
       "             'questioning': 595,\n",
       "             'quite': 596,\n",
       "             'rather': 597,\n",
       "             'rather,': 598,\n",
       "             'rational': 599,\n",
       "             'rationale.': 600,\n",
       "             're-considering': 601,\n",
       "             're-visit': 602,\n",
       "             'real': 603,\n",
       "             'reasons': 604,\n",
       "             'reconsidered.': 605,\n",
       "             'redirect': 606,\n",
       "             'reference.': 607,\n",
       "             'release': 608,\n",
       "             'relevant-this': 609,\n",
       "             'remember': 610,\n",
       "             'remove': 611,\n",
       "             'repeat': 612,\n",
       "             'requesting': 613,\n",
       "             'resolve': 614,\n",
       "             'restatement': 615,\n",
       "             'retired': 616,\n",
       "             'return': 617,\n",
       "             'revert': 618,\n",
       "             'reverted,': 619,\n",
       "             'reverted.': 620,\n",
       "             'reverted?': 621,\n",
       "             'reviewer': 622,\n",
       "             'roc': 623,\n",
       "             'ryo': 624,\n",
       "             'said,': 625,\n",
       "             \"sakazaki's\": 626,\n",
       "             'same': 627,\n",
       "             'santana': 628,\n",
       "             'santanas': 629,\n",
       "             'scalia': 630,\n",
       "             'school': 631,\n",
       "             'section': 632,\n",
       "             'seemingly': 633,\n",
       "             'seems': 634,\n",
       "             'selecting': 635,\n",
       "             'selective': 636,\n",
       "             'self-appointed': 637,\n",
       "             'sense': 638,\n",
       "             'september': 639,\n",
       "             'settled': 640,\n",
       "             'shirvington': 641,\n",
       "             'short': 642,\n",
       "             'signed': 643,\n",
       "             'signpost': 644,\n",
       "             'signpost:': 645,\n",
       "             'similarly': 646,\n",
       "             'single': 647,\n",
       "             'single-page': 648,\n",
       "             'singles': 649,\n",
       "             'sir,': 650,\n",
       "             'site': 651,\n",
       "             'sitting': 652,\n",
       "             'situation': 653,\n",
       "             'sityush': 654,\n",
       "             'six': 655,\n",
       "             'snowflake': 656,\n",
       "             'snowflakes': 657,\n",
       "             'something': 658,\n",
       "             'songs': 659,\n",
       "             'sorry': 660,\n",
       "             'source,': 661,\n",
       "             'specifies': 662,\n",
       "             'stanford': 663,\n",
       "             'start': 664,\n",
       "             'started': 665,\n",
       "             'stated': 666,\n",
       "             'statistics': 667,\n",
       "             'status': 668,\n",
       "             'still': 669,\n",
       "             'strategy': 670,\n",
       "             'strengthen': 671,\n",
       "             'stronger': 672,\n",
       "             'stub.': 673,\n",
       "             'studying': 674,\n",
       "             'style': 675,\n",
       "             'subject': 676,\n",
       "             'subject.': 677,\n",
       "             'subsection': 678,\n",
       "             'sufficient': 679,\n",
       "             'suggestions': 680,\n",
       "             'suggests': 681,\n",
       "             'sullivan': 682,\n",
       "             'support': 683,\n",
       "             'sure': 684,\n",
       "             'surely': 685,\n",
       "             'symmetric': 686,\n",
       "             'symmetrical!': 687,\n",
       "             'tagged': 688,\n",
       "             'tagged,': 689,\n",
       "             'tags#fair': 690,\n",
       "             'taken,': 691,\n",
       "             'talibans...who': 692,\n",
       "             'talk:voydan': 693,\n",
       "             'talked': 694,\n",
       "             'talking': 695,\n",
       "             'template': 696,\n",
       "             'template,': 697,\n",
       "             'terms': 698,\n",
       "             'thanks,': 699,\n",
       "             'thanks.': 700,\n",
       "             'that)': 701,\n",
       "             'them,': 702,\n",
       "             'these': 703,\n",
       "             'thinking': 704,\n",
       "             'third': 705,\n",
       "             'this,': 706,\n",
       "             'thomas': 707,\n",
       "             'through': 708,\n",
       "             'throw': 709,\n",
       "             'throwing': 710,\n",
       "             'tidying': 711,\n",
       "             'too': 712,\n",
       "             'tools': 713,\n",
       "             'top': 714,\n",
       "             'topic': 715,\n",
       "             'tosser.': 716,\n",
       "             'town\"\"': 717,\n",
       "             'true!': 718,\n",
       "             'trying': 719,\n",
       "             'turn': 720,\n",
       "             'turns': 721,\n",
       "             'unattractive': 722,\n",
       "             'unclear.': 723,\n",
       "             'understand': 724,\n",
       "             'unhelpful': 725,\n",
       "             'unnecessary': 726,\n",
       "             'unsourced': 727,\n",
       "             'unspecified': 728,\n",
       "             'unsubscribe': 729,\n",
       "             'untagged': 730,\n",
       "             'until': 731,\n",
       "             'up.': 732,\n",
       "             'uploaded,': 733,\n",
       "             'use)': 734,\n",
       "             'use,': 735,\n",
       "             'username': 736,\n",
       "             'using': 737,\n",
       "             'usually': 738,\n",
       "             'vandalism': 739,\n",
       "             'vandalism),': 740,\n",
       "             'vandalisms,': 741,\n",
       "             'variety.\"\"': 742,\n",
       "             'version': 743,\n",
       "             'video': 744,\n",
       "             'view': 745,\n",
       "             'voted': 746,\n",
       "             'wanting': 747,\n",
       "             'war.': 748,\n",
       "             'warnings': 749,\n",
       "             'warnings...': 750,\n",
       "             'website': 751,\n",
       "             'website,': 752,\n",
       "             'well,': 753,\n",
       "             'well.': 754,\n",
       "             'were': 755,\n",
       "             \"weren't\": 756,\n",
       "             'wikipedia:good_article_nominations#transport': 757,\n",
       "             'willing': 758,\n",
       "             'with.': 759,\n",
       "             'wonder': 760,\n",
       "             'wondered': 761,\n",
       "             'word': 762,\n",
       "             'work': 763,\n",
       "             'wp': 764,\n",
       "             'wp:filmplot': 765,\n",
       "             'wp?': 766,\n",
       "             'wrestling': 767,\n",
       "             'writing': 768,\n",
       "             'writing.': 769,\n",
       "             'written': 770,\n",
       "             'wry': 771,\n",
       "             'yale.': 772,\n",
       "             'yeah,': 773,\n",
       "             'year': 774,\n",
       "             'yes,': 775,\n",
       "             'york': 776,\n",
       "             \"you're\": 777,\n",
       "             'you,': 778,\n",
       "             'yourself': 779,\n",
       "             'yourself,': 780,\n",
       "             'yvesnimmo.': 781,\n",
       "             '|': 782,\n",
       "             '·': 783})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'explanation': 4,\n",
       "         'why': 4,\n",
       "         'the': 78,\n",
       "         'edits': 4,\n",
       "         'made': 2,\n",
       "         'under': 6,\n",
       "         'my': 6,\n",
       "         'username': 1,\n",
       "         'hardcore': 1,\n",
       "         'metallica': 1,\n",
       "         'fan': 1,\n",
       "         'were': 1,\n",
       "         'reverted?': 1,\n",
       "         'they': 5,\n",
       "         \"weren't\": 1,\n",
       "         'vandalisms,': 1,\n",
       "         'just': 2,\n",
       "         'closure': 1,\n",
       "         'on': 16,\n",
       "         'some': 4,\n",
       "         'gas': 1,\n",
       "         'after': 5,\n",
       "         'i': 20,\n",
       "         'voted': 1,\n",
       "         'at': 14,\n",
       "         'new': 2,\n",
       "         'york': 1,\n",
       "         'dolls': 1,\n",
       "         'fac.': 1,\n",
       "         'and': 26,\n",
       "         'please': 9,\n",
       "         \"don't\": 5,\n",
       "         'remove': 1,\n",
       "         'template': 1,\n",
       "         'from': 6,\n",
       "         'talk': 4,\n",
       "         'page': 9,\n",
       "         'since': 2,\n",
       "         \"i'm\": 9,\n",
       "         'retired': 1,\n",
       "         'now.89.205.38.27': 1,\n",
       "         \"d'aww!\": 1,\n",
       "         'he': 10,\n",
       "         'matches': 1,\n",
       "         'this': 17,\n",
       "         'background': 1,\n",
       "         'colour': 1,\n",
       "         'seemingly': 1,\n",
       "         'stuck': 2,\n",
       "         'with.': 1,\n",
       "         'thanks.': 1,\n",
       "         '(talk)': 1,\n",
       "         '21:51,': 1,\n",
       "         'january': 1,\n",
       "         '11,': 1,\n",
       "         '2016': 1,\n",
       "         '(utc)': 1,\n",
       "         'hey': 1,\n",
       "         'man,': 1,\n",
       "         'really': 3,\n",
       "         'not': 11,\n",
       "         'trying': 1,\n",
       "         'to': 41,\n",
       "         'edit': 7,\n",
       "         'war.': 1,\n",
       "         \"it's\": 3,\n",
       "         'that': 22,\n",
       "         'guy': 1,\n",
       "         'is': 24,\n",
       "         'constantly': 1,\n",
       "         'removing': 2,\n",
       "         'relevant': 2,\n",
       "         'information': 1,\n",
       "         'talking': 1,\n",
       "         'me': 8,\n",
       "         'through': 1,\n",
       "         'instead': 1,\n",
       "         'of': 30,\n",
       "         'page.': 4,\n",
       "         'seems': 1,\n",
       "         'care': 1,\n",
       "         'more': 6,\n",
       "         'about': 8,\n",
       "         'formatting': 2,\n",
       "         'than': 3,\n",
       "         'actual': 1,\n",
       "         'info.': 2,\n",
       "         '\"': 16,\n",
       "         \"can't\": 1,\n",
       "         'make': 2,\n",
       "         'any': 10,\n",
       "         'real': 1,\n",
       "         'suggestions': 1,\n",
       "         'improvement': 1,\n",
       "         '-': 4,\n",
       "         'wondered': 1,\n",
       "         'if': 19,\n",
       "         'section': 1,\n",
       "         'statistics': 1,\n",
       "         'should': 3,\n",
       "         'be': 18,\n",
       "         'later': 2,\n",
       "         'on,': 2,\n",
       "         'or': 9,\n",
       "         'a': 26,\n",
       "         'subsection': 1,\n",
       "         '\"\"types': 1,\n",
       "         'accidents\"\"': 1,\n",
       "         '-i': 1,\n",
       "         'think': 4,\n",
       "         'references': 2,\n",
       "         'may': 3,\n",
       "         'need': 3,\n",
       "         'tidying': 1,\n",
       "         'so': 6,\n",
       "         'are': 11,\n",
       "         'all': 1,\n",
       "         'in': 17,\n",
       "         'exact': 1,\n",
       "         'same': 1,\n",
       "         'format': 2,\n",
       "         'ie': 1,\n",
       "         'date': 1,\n",
       "         'etc.': 1,\n",
       "         'can': 10,\n",
       "         'do': 7,\n",
       "         'no-one': 1,\n",
       "         'else': 1,\n",
       "         'does': 1,\n",
       "         'first': 2,\n",
       "         'you': 33,\n",
       "         'have': 16,\n",
       "         'preferences': 1,\n",
       "         'for': 13,\n",
       "         'style': 1,\n",
       "         'want': 2,\n",
       "         'it': 16,\n",
       "         'yourself': 1,\n",
       "         'let': 1,\n",
       "         'know.': 2,\n",
       "         'there': 5,\n",
       "         'appears': 1,\n",
       "         'backlog': 1,\n",
       "         'articles': 4,\n",
       "         'review': 2,\n",
       "         'guess': 1,\n",
       "         'delay': 1,\n",
       "         'until': 1,\n",
       "         'reviewer': 1,\n",
       "         'turns': 1,\n",
       "         'up.': 1,\n",
       "         'listed': 3,\n",
       "         'form': 1,\n",
       "         'eg': 1,\n",
       "         'wikipedia:good_article_nominations#transport': 1,\n",
       "         'you,': 1,\n",
       "         'sir,': 1,\n",
       "         'hero.': 1,\n",
       "         'chance': 1,\n",
       "         'remember': 1,\n",
       "         'what': 4,\n",
       "         \"that's\": 2,\n",
       "         'on?': 1,\n",
       "         'congratulations': 1,\n",
       "         'as': 10,\n",
       "         'well,': 1,\n",
       "         'use': 12,\n",
       "         'tools': 1,\n",
       "         'well.': 1,\n",
       "         '·': 1,\n",
       "         'cocksucker': 1,\n",
       "         'before': 4,\n",
       "         'piss': 1,\n",
       "         'around': 3,\n",
       "         'work': 1,\n",
       "         'your': 7,\n",
       "         'vandalism': 1,\n",
       "         'matt': 1,\n",
       "         'shirvington': 1,\n",
       "         'article': 5,\n",
       "         'has': 2,\n",
       "         'been': 5,\n",
       "         'reverted.': 1,\n",
       "         'again,': 2,\n",
       "         'will': 5,\n",
       "         'banned.': 1,\n",
       "         'sorry': 1,\n",
       "         'word': 1,\n",
       "         \"'nonsense'\": 1,\n",
       "         'was': 6,\n",
       "         'offensive': 1,\n",
       "         'you.': 3,\n",
       "         'anyway,': 1,\n",
       "         'intending': 1,\n",
       "         'write': 2,\n",
       "         'anything': 3,\n",
       "         'article(wow': 1,\n",
       "         'would': 4,\n",
       "         'jump': 1,\n",
       "         'vandalism),': 1,\n",
       "         'merely': 2,\n",
       "         'requesting': 1,\n",
       "         'encyclopedic': 1,\n",
       "         'one': 8,\n",
       "         'school': 1,\n",
       "         'reference.': 1,\n",
       "         'selective': 1,\n",
       "         'breeding': 1,\n",
       "         'but': 5,\n",
       "         'almost': 1,\n",
       "         'stub.': 1,\n",
       "         'points': 1,\n",
       "         \"'animal\": 1,\n",
       "         \"breeding'\": 1,\n",
       "         'which': 4,\n",
       "         'short': 1,\n",
       "         'messy': 1,\n",
       "         'gives': 1,\n",
       "         'no': 4,\n",
       "         'must': 2,\n",
       "         'someone': 2,\n",
       "         'with': 9,\n",
       "         'expertise': 1,\n",
       "         'eugenics?': 1,\n",
       "         '93.161.107.169': 1,\n",
       "         'alignment': 1,\n",
       "         'subject': 1,\n",
       "         'contrary': 1,\n",
       "         'those': 3,\n",
       "         'dulithgow': 1,\n",
       "         'fair': 9,\n",
       "         'rationale': 4,\n",
       "         'image:wonju.jpg': 2,\n",
       "         'thanks': 2,\n",
       "         'uploading': 2,\n",
       "         'image:wonju.jpg.': 2,\n",
       "         'notice': 1,\n",
       "         'image': 7,\n",
       "         'specifies': 1,\n",
       "         'being': 1,\n",
       "         'used': 2,\n",
       "         'its': 2,\n",
       "         'wikipedia': 3,\n",
       "         'constitutes': 1,\n",
       "         'use.': 4,\n",
       "         'addition': 1,\n",
       "         'boilerplate': 1,\n",
       "         'template,': 1,\n",
       "         'also': 3,\n",
       "         'out': 2,\n",
       "         'description': 3,\n",
       "         'specific': 2,\n",
       "         'using': 1,\n",
       "         'each': 1,\n",
       "         'consistent': 1,\n",
       "         'go': 2,\n",
       "         'include': 3,\n",
       "         'rationale.': 1,\n",
       "         'uploaded': 4,\n",
       "         'other': 4,\n",
       "         'media,': 1,\n",
       "         'consider': 2,\n",
       "         'checking': 2,\n",
       "         'specified': 2,\n",
       "         'pages': 2,\n",
       "         'too.': 2,\n",
       "         'find': 2,\n",
       "         'list': 3,\n",
       "         \"'image'\": 1,\n",
       "         'edited': 1,\n",
       "         'by': 4,\n",
       "         'clicking': 1,\n",
       "         '\"\"my': 1,\n",
       "         'contributions\"\"': 1,\n",
       "         'link': 2,\n",
       "         '(it': 1,\n",
       "         'located': 1,\n",
       "         'very': 2,\n",
       "         'top': 1,\n",
       "         'when': 4,\n",
       "         'logged': 1,\n",
       "         'in),': 1,\n",
       "         'then': 9,\n",
       "         'selecting': 1,\n",
       "         '\"\"image\"\"': 1,\n",
       "         'dropdown': 1,\n",
       "         'box.': 1,\n",
       "         'note': 2,\n",
       "         'images': 2,\n",
       "         '4': 1,\n",
       "         'may,': 1,\n",
       "         '2006,': 1,\n",
       "         'lacking': 1,\n",
       "         'such': 2,\n",
       "         'an': 7,\n",
       "         'deleted': 3,\n",
       "         'week': 2,\n",
       "         'uploaded,': 1,\n",
       "         'described': 2,\n",
       "         'criteria': 3,\n",
       "         'speedy': 2,\n",
       "         'deletion.': 2,\n",
       "         'questions': 5,\n",
       "         'ask': 4,\n",
       "         'them': 4,\n",
       "         'media': 3,\n",
       "         'copyright': 9,\n",
       "         'thank': 2,\n",
       "         '(talk': 2,\n",
       "         '•': 4,\n",
       "         'contribs': 2,\n",
       "         ')': 2,\n",
       "         'unspecified': 1,\n",
       "         'source': 2,\n",
       "         'noticed': 1,\n",
       "         \"file's\": 1,\n",
       "         'currently': 1,\n",
       "         \"doesn't\": 3,\n",
       "         'specify': 2,\n",
       "         'who': 4,\n",
       "         'created': 1,\n",
       "         'content,': 2,\n",
       "         'status': 1,\n",
       "         'unclear.': 1,\n",
       "         'did': 2,\n",
       "         'create': 1,\n",
       "         'file': 2,\n",
       "         'yourself,': 1,\n",
       "         'owner': 1,\n",
       "         'copyright.': 1,\n",
       "         'obtained': 1,\n",
       "         'website,': 1,\n",
       "         'website': 1,\n",
       "         'taken,': 1,\n",
       "         'together': 2,\n",
       "         'restatement': 1,\n",
       "         \"website's\": 2,\n",
       "         'terms': 1,\n",
       "         'usually': 1,\n",
       "         'sufficient': 1,\n",
       "         'information.': 1,\n",
       "         'however,': 1,\n",
       "         'holder': 1,\n",
       "         'different': 1,\n",
       "         'publisher,': 1,\n",
       "         'their': 3,\n",
       "         'acknowledged.': 1,\n",
       "         'well': 2,\n",
       "         'adding': 1,\n",
       "         'source,': 1,\n",
       "         'add': 1,\n",
       "         'proper': 1,\n",
       "         'licensing': 1,\n",
       "         'tag': 3,\n",
       "         'already.': 1,\n",
       "         'created/took': 1,\n",
       "         'picture,': 1,\n",
       "         'audio,': 1,\n",
       "         'video': 1,\n",
       "         'release': 1,\n",
       "         'gfdl.': 1,\n",
       "         'believe': 3,\n",
       "         'meets': 1,\n",
       "         'wikipedia:fair': 2,\n",
       "         'use,': 1,\n",
       "         'tags': 3,\n",
       "         'wikipedia:image': 2,\n",
       "         'tags#fair': 1,\n",
       "         'see': 3,\n",
       "         'full': 2,\n",
       "         'files,': 1,\n",
       "         'tagged': 1,\n",
       "         'them,': 1,\n",
       "         'files': 1,\n",
       "         'following': 2,\n",
       "         '[': 1,\n",
       "         'link].': 1,\n",
       "         'unsourced': 1,\n",
       "         'untagged': 1,\n",
       "         'tagged,': 1,\n",
       "         'copyrighted': 1,\n",
       "         'non-free': 1,\n",
       "         'license': 1,\n",
       "         '(per': 1,\n",
       "         'use)': 1,\n",
       "         '48': 1,\n",
       "         'hours': 1,\n",
       "         '.': 2,\n",
       "         'bbq': 1,\n",
       "         'man': 1,\n",
       "         'lets': 2,\n",
       "         'discuss': 2,\n",
       "         'it-maybe': 1,\n",
       "         'over': 1,\n",
       "         'phone?': 1,\n",
       "         'hey...': 1,\n",
       "         'it..': 1,\n",
       "         '@': 1,\n",
       "         '|': 1,\n",
       "         'it...': 1,\n",
       "         'exclusive': 1,\n",
       "         'group': 1,\n",
       "         'wp': 1,\n",
       "         'talibans...who': 1,\n",
       "         'good': 2,\n",
       "         'destroying,': 1,\n",
       "         'self-appointed': 1,\n",
       "         'purist': 1,\n",
       "         'gang': 1,\n",
       "         'up': 2,\n",
       "         'asks': 1,\n",
       "         'abt': 1,\n",
       "         'anti-social': 1,\n",
       "         'destructive': 1,\n",
       "         '(non)-contribution': 1,\n",
       "         'wp?': 1,\n",
       "         'sityush': 1,\n",
       "         'clean': 1,\n",
       "         'his': 8,\n",
       "         'behavior': 1,\n",
       "         'issue': 2,\n",
       "         'nonsensical': 1,\n",
       "         'warnings...': 1,\n",
       "         'start': 1,\n",
       "         'throwing': 1,\n",
       "         'accusations': 2,\n",
       "         'warnings': 1,\n",
       "         'me,': 2,\n",
       "         'itself-making': 1,\n",
       "         'ad': 1,\n",
       "         'hominem': 1,\n",
       "         'attacks': 1,\n",
       "         \"isn't\": 1,\n",
       "         'going': 1,\n",
       "         'strengthen': 1,\n",
       "         'argument,': 1,\n",
       "         'look': 4,\n",
       "         'like': 3,\n",
       "         'abusing': 1,\n",
       "         'power': 1,\n",
       "         'admin.': 1,\n",
       "         'now,': 1,\n",
       "         'itself': 1,\n",
       "         'relevant-this': 1,\n",
       "         'probably': 1,\n",
       "         'single': 1,\n",
       "         'most': 2,\n",
       "         'talked': 1,\n",
       "         'event': 1,\n",
       "         'int': 1,\n",
       "         'news': 1,\n",
       "         'late.': 1,\n",
       "         'absence': 1,\n",
       "         'notable,': 1,\n",
       "         'only': 1,\n",
       "         'living': 1,\n",
       "         'ex-president': 1,\n",
       "         'attend.': 1,\n",
       "         'certainly': 1,\n",
       "         'notable': 1,\n",
       "         'dedicating': 1,\n",
       "         'aircracft': 1,\n",
       "         'carrier.': 1,\n",
       "         'intend': 1,\n",
       "         'revert': 1,\n",
       "         'edit,': 1,\n",
       "         'hopes': 1,\n",
       "         'attracting': 1,\n",
       "         'attention': 1,\n",
       "         'admin': 1,\n",
       "         'willing': 1,\n",
       "         'itself,': 1,\n",
       "         'throw': 1,\n",
       "         'quite': 1,\n",
       "         'liberally.': 1,\n",
       "         'perhaps,': 1,\n",
       "         'achieve': 1,\n",
       "         'level': 1,\n",
       "         'civility': 1,\n",
       "         'where': 2,\n",
       "         'this,': 1,\n",
       "         'we': 2,\n",
       "         'rational': 1,\n",
       "         'discussion': 1,\n",
       "         'topic': 1,\n",
       "         'resolve': 1,\n",
       "         'matter': 1,\n",
       "         'peacefully.': 1,\n",
       "         'oh,': 1,\n",
       "         'girl': 1,\n",
       "         'above': 1,\n",
       "         'started': 1,\n",
       "         'her': 2,\n",
       "         'arguments': 1,\n",
       "         'me.': 1,\n",
       "         'she': 2,\n",
       "         'nose': 1,\n",
       "         'belong.': 1,\n",
       "         'argument': 3,\n",
       "         'between': 1,\n",
       "         'yvesnimmo.': 1,\n",
       "         'said,': 1,\n",
       "         'situation': 1,\n",
       "         'settled': 1,\n",
       "         'apologized.': 1,\n",
       "         'thanks,': 1,\n",
       "         'juelz': 3,\n",
       "         'santanas': 1,\n",
       "         'age': 1,\n",
       "         '2002,': 1,\n",
       "         'santana': 1,\n",
       "         '18': 1,\n",
       "         'years': 3,\n",
       "         'old,': 1,\n",
       "         'came': 1,\n",
       "         'february': 1,\n",
       "         '18th,': 1,\n",
       "         'makes': 3,\n",
       "         'turn': 1,\n",
       "         '19': 1,\n",
       "         'making': 1,\n",
       "         'songs': 1,\n",
       "         'diplomats.': 1,\n",
       "         'third': 1,\n",
       "         'neff': 2,\n",
       "         'signed': 1,\n",
       "         \"cam's\": 1,\n",
       "         'label': 1,\n",
       "         'roc': 1,\n",
       "         'fella.': 1,\n",
       "         '2003,': 1,\n",
       "         '20': 1,\n",
       "         'old': 1,\n",
       "         'coming': 1,\n",
       "         'own': 1,\n",
       "         'singles': 1,\n",
       "         '\"\"santana\\'s': 1,\n",
       "         'town\"\"': 1,\n",
       "         '\"\"down\"\".': 1,\n",
       "         'yes,': 1,\n",
       "         'born': 1,\n",
       "         '1983.': 1,\n",
       "         'is,': 1,\n",
       "         'how': 3,\n",
       "         'could': 2,\n",
       "         'older': 1,\n",
       "         'lloyd': 1,\n",
       "         'banks?': 1,\n",
       "         '22': 1,\n",
       "         'birthday': 1,\n",
       "         'passed?': 1,\n",
       "         'homie': 1,\n",
       "         '23': 1,\n",
       "         'old.': 1,\n",
       "         '1983': 1,\n",
       "         '2006': 1,\n",
       "         '(juelz': 1,\n",
       "         'death,': 1,\n",
       "         'god': 1,\n",
       "         'forbid': 1,\n",
       "         'thinking': 1,\n",
       "         'that)': 1,\n",
       "         'equals': 1,\n",
       "         '23.': 1,\n",
       "         'caculator': 1,\n",
       "         'stop': 2,\n",
       "         'changing': 1,\n",
       "         'year': 1,\n",
       "         'birth.': 1,\n",
       "         'god.\"': 1,\n",
       "         'bye!': 1,\n",
       "         'look,': 1,\n",
       "         'come': 1,\n",
       "         'comming': 1,\n",
       "         'back!': 1,\n",
       "         'tosser.': 1,\n",
       "         'redirect': 1,\n",
       "         'talk:voydan': 1,\n",
       "         'pop': 1,\n",
       "         'georgiev-': 1,\n",
       "         'chernodrinski': 1,\n",
       "         'mitsurugi': 1,\n",
       "         'point': 1,\n",
       "         'sense': 1,\n",
       "         'argue': 1,\n",
       "         'hindi': 1,\n",
       "         'ryo': 1,\n",
       "         \"sakazaki's\": 1,\n",
       "         'information?': 1,\n",
       "         'mean': 1,\n",
       "         'bother': 1,\n",
       "         \"you're\": 1,\n",
       "         'writing': 1,\n",
       "         'something': 1,\n",
       "         'regarding': 2,\n",
       "         'posted': 1,\n",
       "         'here': 1,\n",
       "         'oh': 1,\n",
       "         'acctually': 1,\n",
       "         'even': 2,\n",
       "         'better.': 1,\n",
       "         \"i'd\": 1,\n",
       "         'take': 2,\n",
       "         'closer': 1,\n",
       "         'premature': 1,\n",
       "         'wrestling': 1,\n",
       "         'deaths': 1,\n",
       "         'catagory': 2,\n",
       "         'men': 2,\n",
       "         'it,': 1,\n",
       "         'surely': 1,\n",
       "         'these': 1,\n",
       "         'belong': 1,\n",
       "         'catagory.': 1,\n",
       "         'besides': 1,\n",
       "         'delting': 1,\n",
       "         'it?': 1,\n",
       "         'recent': 2,\n",
       "         'once': 1,\n",
       "         'read': 2,\n",
       "         'wp:filmplot': 1,\n",
       "         'editing': 1,\n",
       "         'film': 1,\n",
       "         'articles.': 1,\n",
       "         'simply': 2,\n",
       "         'good,': 1,\n",
       "         'entirely': 1,\n",
       "         'too': 1,\n",
       "         'many': 1,\n",
       "         'unnecessary': 1,\n",
       "         'details': 1,\n",
       "         'bad': 1,\n",
       "         'writing.': 1,\n",
       "         'further': 1,\n",
       "         'damage.': 1,\n",
       "         \"-''''''the\": 1,\n",
       "         \"'45\": 1,\n",
       "         'yeah,': 1,\n",
       "         'studying': 1,\n",
       "         'now.(deepu)': 1,\n",
       "         'snowflakes': 1,\n",
       "         'always': 2,\n",
       "         'symmetrical!': 1,\n",
       "         'geometry': 1,\n",
       "         'stated': 1,\n",
       "         '\"\"a': 1,\n",
       "         'snowflake': 1,\n",
       "         'six': 1,\n",
       "         'symmetric': 1,\n",
       "         'arms.\"\"': 1,\n",
       "         'assertion': 1,\n",
       "         'true!': 1,\n",
       "         'according': 1,\n",
       "         'kenneth': 1,\n",
       "         'libbrecht,': 1,\n",
       "         '\"\"the': 1,\n",
       "         'rather': 1,\n",
       "         'unattractive': 1,\n",
       "         'irregular': 1,\n",
       "         'crystals': 1,\n",
       "         'far': 1,\n",
       "         'common': 1,\n",
       "         'variety.\"\"': 1,\n",
       "         'http://www.its.caltech.edu/~atomic/snowcrystals/myths/myths.htm#perfection': 1,\n",
       "         'site': 1,\n",
       "         'get': 1,\n",
       "         'facts': 1,\n",
       "         'off': 1,\n",
       "         'because': 1,\n",
       "         'still': 1,\n",
       "         'decent': 1,\n",
       "         'number': 1,\n",
       "         'falsities': 1,\n",
       "         '(forgive': 1,\n",
       "         'im': 1,\n",
       "         'dont': 1,\n",
       "         'anything)\"': 1,\n",
       "         'signpost:': 1,\n",
       "         '24': 1,\n",
       "         'september': 1,\n",
       "         '2012': 1,\n",
       "         'signpost': 1,\n",
       "         'single-page': 1,\n",
       "         'unsubscribe': 1,\n",
       "         're-considering': 1,\n",
       "         '1st': 1,\n",
       "         'paragraph': 2,\n",
       "         'edit?': 2,\n",
       "         'understand': 1,\n",
       "         'reasons': 1,\n",
       "         \"'s\": 1,\n",
       "         'sure': 1,\n",
       "         'data': 1,\n",
       "         'necessarily': 1,\n",
       "         '\"\"wrong.\"\"': 1,\n",
       "         'rather,': 1,\n",
       "         'persuaded': 1,\n",
       "         'strategy': 1,\n",
       "         'introducing': 1,\n",
       "         'academic': 2,\n",
       "         'honors': 1,\n",
       "         'unhelpful': 1,\n",
       "         'approach': 1,\n",
       "         'subject.': 1,\n",
       "         'sitting': 1,\n",
       "         'justices': 1,\n",
       "         'similarly': 1,\n",
       "         '\"\"enhanced;\"\"': 1,\n",
       "         'changes': 1,\n",
       "         'improvement.': 1,\n",
       "         'support': 1,\n",
       "         'view': 1,\n",
       "         'reverted,': 1,\n",
       "         'invite': 1,\n",
       "         'anyone': 1,\n",
       "         're-visit': 1,\n",
       "         'written': 1,\n",
       "         'pairs': 1,\n",
       "         'jurists.': 1,\n",
       "         'a1.': 1,\n",
       "         'benjamin': 1,\n",
       "         'cardozo': 1,\n",
       "         'a2.': 1,\n",
       "         'learned': 1,\n",
       "         'hand': 1,\n",
       "         'b1.': 1,\n",
       "         'john': 2,\n",
       "         'marshall': 2,\n",
       "         'harlan': 2,\n",
       "         'b2.': 1,\n",
       "         'ii': 1,\n",
       "         'question': 1,\n",
       "         'becomes:': 1,\n",
       "         'current': 1,\n",
       "         'version': 1,\n",
       "         'either': 1,\n",
       "         'pair': 1,\n",
       "         'improved': 1,\n",
       "         'credentials': 1,\n",
       "         'introductory': 1,\n",
       "         'paragraph?': 1,\n",
       "         'not.': 1,\n",
       "         'perhaps': 1,\n",
       "         'helps': 1,\n",
       "         'repeat': 1,\n",
       "         'wry': 1,\n",
       "         'kathleen': 1,\n",
       "         'sullivan': 1,\n",
       "         'stanford': 1,\n",
       "         'law': 3,\n",
       "         'suggests': 1,\n",
       "         'harvard': 1,\n",
       "         'faculty': 1,\n",
       "         'wonder': 1,\n",
       "         'antonin': 1,\n",
       "         'scalia': 1,\n",
       "         'avoided': 1,\n",
       "         'learning': 1,\n",
       "         'others': 1,\n",
       "         'managed': 1,\n",
       "         'grasp': 1,\n",
       "         'processes': 1,\n",
       "         'judging?': 1,\n",
       "         'hope': 1,\n",
       "         'anecdote': 1,\n",
       "         'gently': 1,\n",
       "         'illustrates': 1,\n",
       "         'point.': 1,\n",
       "         'less': 1,\n",
       "         'humorous,': 1,\n",
       "         'stronger': 1,\n",
       "         'clarence': 1,\n",
       "         'thomas': 1,\n",
       "         'mentions': 1,\n",
       "         'wanting': 1,\n",
       "         'return': 1,\n",
       "         'degree': 1,\n",
       "         'yale.': 1,\n",
       "         'minimum,': 1,\n",
       "         'questioning': 1,\n",
       "         'deserves': 1,\n",
       "         'reconsidered.': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab.freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the two difference between 782 and 784 may be from '<unk>': 0, '<pad>': 1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also instructive to take a look inside the Dataset. Datasets can be indexed like normal lists, so we'll look at the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the dataset is an Example object that bundles the attributes of a single data point together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f2f0c4c5748>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5780>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c57b8>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c57f0>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5828>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5860>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5898>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c58d0>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5908>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5940>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5978>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c59b0>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c59e8>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5a20>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5a58>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5a90>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5b38>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c5be0>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c86a0>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c86d8>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c8710>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c87b8>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4c8e10>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4ca6d8>,\n",
       " <torchtext.data.example.Example at 0x7f2f0c4ca710>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.__dict__['examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['explanation', 'why', 'the', 'edits', 'made', 'under', 'my', 'username', 'hardcore', 'metallica', 'fan', 'were', 'reverted?', 'they', \"weren't\", 'vandalisms,', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fac.', 'and', 'please', \"don't\", 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', \"i'm\", 'retired', 'now.89.205.38.27'], '0', '0', '0', '0', '0', '0'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the comment text is already tokenized for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation', 'why', 'the']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].comment_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good. Now, let's build the Iterator which will allow us to load the data into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**.\n",
    "\n",
    "When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time.\n",
    "\n",
    "The BucketIterator groups sequences of similar lengths together for each batch to minimize padding. Handy, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    "#         batch_sizes=(64, 64),\n",
    "        batch_sizes=(23,23),\n",
    "#         device=torch.device, # if you want to use the GPU, specify the GPU number here\n",
    "        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the output of the BucketIterator looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 23]\n",
       "\t[.comment_text]:[torch.LongTensor of size 228x23]\n",
       "\t[.toxic]:[torch.LongTensor of size 23]\n",
       "\t[.severe_toxic]:[torch.LongTensor of size 23]\n",
       "\t[.threat]:[torch.LongTensor of size 23]\n",
       "\t[.obscene]:[torch.LongTensor of size 23]\n",
       "\t[.insult]:[torch.LongTensor of size 23]\n",
       "\t[.identity_hate]:[torch.LongTensor of size 23]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, we don't want the data to be shuffled. This is why we'll be using a standard Iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = Iterator(tst, batch_size=20,sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the iterator returns a custom datatype called torchtext.data.Batch. This makes code reuse difficult (since each time the column names change, we need to modify the code), and makes torchtext hard to use with other libraries for some use cases (like torchsample and fastai). \n",
    "\n",
    "I hope this will be dealt with in the future (I'm considering filing a PR if I can decide what the API should look like), but in the meantime, we'll hack on a simple wrapper to make the batches easy to use. \n",
    "\n",
    "Concretely, we'll convert the batch to a tuple in the form (x, y) where x is the independent variable (the input to the model) and y is the dependent variable (the supervision data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (<ipython-input-128-22c96d9e1421>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-128-22c96d9e1421>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    yield (x, y)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "\n",
    "    if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "        y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "    else:\n",
    "        y = torch.zeros((1))\n",
    "\n",
    "    yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15, 453,  55,  ...,  15, 660, 280],\n",
       "        [360, 523, 531,  ...,  29,  11,  18],\n",
       "        [ 45,  30,   3,  ...,  21,   2,  14],\n",
       "        ...,\n",
       "        [  1,   1,   1,  ...,  84,   1,   1],\n",
       "        [  1,   1,   1,  ..., 118,   1,   1],\n",
       "        [  1,   1,   1,  ...,  15,   1,   1]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=next(train_iter.__iter__())\n",
    "a.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([494, 23])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.comment_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 23]\n",
       "\t[.comment_text]:[torch.LongTensor of size 494x23]\n",
       "\t[.toxic]:[torch.LongTensor of size 23]\n",
       "\t[.severe_toxic]:[torch.LongTensor of size 23]\n",
       "\t[.threat]:[torch.LongTensor of size 23]\n",
       "\t[.obscene]:[torch.LongTensor of size 23]\n",
       "\t[.insult]:[torch.LongTensor of size 23]\n",
       "\t[.identity_hate]:[torch.LongTensor of size 23]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function getattr>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this to wrap the BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "valid_dl = BatchWrapper(val_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "test_dl = BatchWrapper(test_iter, \"comment_text\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 15, 606,  15,  ...,  44, 375, 354],\n",
       "         [ 46, 693, 601,  ..., 739,  27,  63],\n",
       "         [ 10, 584, 242,  ...,   3, 526,   4],\n",
       "         ...,\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 1., 1., 0.]]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=next(train_dl.__iter__())[0]\n",
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([113, 5])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start training a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2d424489189b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "next(train_dl.__iter__())[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a simple LSTM as a baseline example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(784, 100)\n",
       "  (encoder): LSTM(100, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList()\n",
       "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz); model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a GPU, remember to call model.cuda() to move your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 266.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 63, 315, 453,  ..., 280, 334,  44],\n",
      "        [  4,  12, 523,  ...,  18,  55, 739],\n",
      "        [664,   6,  30,  ...,  14, 520,   3],\n",
      "        ...,\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1]])\n",
      "torch.Size([494, 23])\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([23, 6])\n",
      "tensor([[ 15,  15],\n",
      "        [178, 144],\n",
      "        [ 44,   3],\n",
      "        [176, 156],\n",
      "        [ 65,  36],\n",
      "        [564, 163],\n",
      "        [122, 773],\n",
      "        [ 33,  30],\n",
      "        [175, 674],\n",
      "        [765, 551],\n",
      "        [ 63,  15],\n",
      "        [401,   1],\n",
      "        [ 24,   1],\n",
      "        [ 46,   1],\n",
      "        [426,   1],\n",
      "        [298,   1],\n",
      "        [ 44,   1],\n",
      "        [ 65,   1],\n",
      "        [ 22,   1],\n",
      "        [183,   1],\n",
      "        [ 23,   1],\n",
      "        [442,   1],\n",
      "        [ 35,   1],\n",
      "        [406,   1],\n",
      "        [712,   1],\n",
      "        [525,   1],\n",
      "        [726,   1],\n",
      "        [389,   1],\n",
      "        [  7,   1],\n",
      "        [203,   1],\n",
      "        [312,   1],\n",
      "        [769,   1],\n",
      "        [ 33,   1],\n",
      "        [191,   1],\n",
      "        [ 63,   1],\n",
      "        [  4,   1],\n",
      "        [ 41,   1],\n",
      "        [429,   1],\n",
      "        [376,   1],\n",
      "        [234,   1],\n",
      "        [222,   1],\n",
      "        [ 15,   1]])\n",
      "torch.Size([42, 2])\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in tqdm.tqdm(train_dl):\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:21<00:00,  6.77s/it]\u001b[A\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 6.9697, Validation Loss: 2.0425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:07<00:07,  7.50s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:08<00:00,  5.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 16.9768, Validation Loss: 1.7631\n",
      "CPU times: user 56.4 s, sys: 268 ms, total: 56.6 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm.tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we output the data in the format required by the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BatchWrapper at 0x7f2f0c3665c0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.5030732  0.5207203  0.475422   0.5145852  0.48815513 0.49939278]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.48426753 0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]]\n",
      "(20, 6)\n",
      "[[0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.48426753 0.47546154 0.52316993]\n",
      " [0.49442008 0.51475674 0.46904233 0.52110183 0.47865188 0.5059334 ]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.5249471  0.49375692 0.48426753 0.47546145 0.5231699 ]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953184 0.52494997 0.493754   0.4842682  0.47546035 0.5231678 ]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375698 0.4842676  0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375692 0.48426753 0.47546154 0.52316993]\n",
      " [0.49953315 0.524947   0.49375692 0.4842676  0.47546154 0.52316993]]\n",
      "(13, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-3447a0b74999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/squad/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for x, y in tqdm.tqdm(test_dl):\n",
    "    preds = model(x)\n",
    "    # if you're data is on the GPU, you need to move the data back to the cpu\n",
    "    # preds = preds.data.cpu().numpy()\n",
    "    preds = preds.data.numpy()\n",
    "    # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "    print(preds)\n",
    "    print(preds.shape)\n",
    "test_preds = np.hstack(test_preds)\n",
    "print(test_preds)\n",
    "print(test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-4348beac2fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"toxic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"severe_toxic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"obscene\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"threat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"insult\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"identity_hate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# if you want to write the submission file to disk, uncomment and run the below code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/test.csv\")\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    df[col] = test_preds[:, i]\n",
    "\n",
    "# if you want to write the submission file to disk, uncomment and run the below code\n",
    "# df.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.015002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.013003</td>\n",
       "      <td>0.01686</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.015002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.013003</td>\n",
       "      <td>0.01686</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.015002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.013003</td>\n",
       "      <td>0.01686</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "\n",
       "    toxic  severe_toxic   obscene    threat   insult  identity_hate  \n",
       "0  0.1479      0.015002  0.000021  0.013003  0.01686       0.000025  \n",
       "1  0.1479      0.015002  0.000021  0.013003  0.01686       0.000025  \n",
       "2  0.1479      0.015002  0.000021  0.013003  0.01686       0.000025  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
